
@doc raw"""
Tucker{N, R, D, ğ”½} <: AbstractManifold{ğ”½}

The manifold of $N_1 \times \dots \times N_D$ real-valued or complex-valued tensors of
fixed multilinear rank $(R_1, \dots, R_D)$ . If $R_1 = \dots = R_D = 1$, this is the
manifold of rank-1 tensors.

# Representation in HOSVD format

Any tensor $\mathcal{A}$ on the Tucker manifold can be represented in HOSVD
[^DeLathauwer2000] form
```math
\mathcal{A} = (U_1,\dots,U_D) \cdot \mathcal{C}
```
where $\mathcal C \in \mathbb{F}^{R_1 \times \dots \times R_D}$ and, for $d=1,\dots,D$,
the matrix $U_d \in \mathbb{F}^{N_d \times R_d}$ contains the singular vectors of the
$d$th unfolding of $\mathcal{A}$

# Tangent space

The tangent space to the Tucker manifold at
$\mathcal{A} = (U_1,\dots,U_D) \cdot \mathcal{C}$ is [^Koch2010]
```math
T_{\mathcal{A}} \mathcal{M} =
\bigl\{
(U_1,\dots,U_D) \cdot \dot{\mathcal{C}}
+ \sum_{d=1}^D \bigl(
    (U_1, \dots, U_{d-1}, \dot{U}_d, U_{d+1}, \dots, U_D)
    \cdot \mathcal{C}
\bigr)
\bigr\}
```
where $\dot{\mathcal{C}}$ is arbitrary and $\dot{U}_d^{\mathrm{H}} U_d = 0$ for all $d$.

# Constructor
Tucker(nâƒ— :: NTuple{D, Int}, râƒ— :: NTuple{D, Int}[, field = â„])

Generate the manifold of $N_1 \times \dots \times N_D$ tensors of fixed multilinear rank
$(R_1, \dots, R_D)$

[^DeLathauwer2000]:
> Lieven De Lathauwer, Bart De Moor, Joos Vandewalle: "A multilinear singular value decomposition"
> SIAM Journal on Matrix Analysis and Applications, 21(4), pp. 1253-1278, 2000
> doi: [10.1137/S0895479896305696](https://doi.org/10.1137/S0895479896305696)

[^Koch2010]:
> Othmar Koch, Christian Lubic, "Dynamical Tensor approximation"
> SIAM Journal on Matrix Analysis and Applications, 31(5), pp. 2360-2375, 2010
> doi: [10.1137/09076578X](https://doi.org/10.1137/09076578X)
"""
struct Tucker{N,R,D,ğ”½} <: AbstractManifold{ğ”½} end
function Tucker(nâƒ—::NTuple{D,Int}, râƒ—::NTuple{D,Int}, field::AbstractNumbers=â„) where {D}
    @assert is_valid_mlrank(nâƒ—, râƒ—)
    return Tucker{nâƒ—,râƒ—,D,field}()
end

#=
HOSVD{T, D}

Higher-order singular value decomposition of an order D tensor with eltype T
fields:
* U: singular vectors of the unfoldings
* core: core tensor
* Ïƒ : singular values of the unfoldings
=#
struct HOSVD{T,D}
    U::NTuple{D,Matrix{T}}
    core::Array{T,D}
    Ïƒ::NTuple{D,Vector{T}}
end

"""
TuckerPoint{T, D}

An order D tensor of fixed multilinear rank and entries of type T. The tensor is
represented in HOSVD form. See also [`Tucker`](@ref).

# Constructors:
TuckerPoint(core :: AbstractArray{T, D}, factors :: Vararg{MtxT, D}) where {T, D, MtxT <: AbstractMatrix{T}}

A tensor of the form (factors[1], â€¦, factors[D]) â‹… core
where it is assumed that the dimensions of the core are the multilinear rank of the tensor.

TuckerPoint(A :: AbstractArray{T, D}, mlrank :: NTuple{D, Int}) where {T, D}

The low-multilinear rank tensor arising from the sequentially truncated the higher-order
singular value decomposition of A [^Vannieuwenhoven2012].

[^Vannieuwenhoven2012]:
> Nick Vannieuwenhoven, Raf Vandebril, Karl Meerbergen: "A new truncation strategy for the higher-order singular value decomposition"
> SIAM Journal on Scientific Computing, 34(2), pp. 1027-1052, 2012
> doi: [10.1137/110836067](https://doi.org/10.1137/110836067)

"""
struct TuckerPoint{T,D} <: AbstractManifoldPoint
    hosvd::HOSVD{T,D}
end
function TuckerPoint(
    core::AbstractArray{T,D},
    factors::Vararg{MtxT,D},
) where {T,D,MtxT<:AbstractMatrix{T}}
    # Take the QR decompositions of the factors and multiply the R factors into the core
    qrfacs = qr.(factors)
    Q = map(qrfac -> qrfac.Q, qrfacs)
    R = map(qrfac -> qrfac.R, qrfacs)
    coreâ€² = reshape(Kronecker.:âŠ—(reverse(R)...) * vec(core), size(core))

    # Convert to HOSVD format by taking the HOSVD of the core
    decomp = st_hosvd(coreâ€²)
    factorsâ€² = Q .* decomp.U
    return TuckerPoint(HOSVD{T,D}(factorsâ€², decomp.core, decomp.Ïƒ))
end
function TuckerPoint(A::AbstractArray{T,D}, mlrank::NTuple{D,Int}) where {T,D}
    @assert is_valid_mlrank(size(A), mlrank)
    return TuckerPoint(st_hosvd(A, mlrank))
end

@doc raw"""
TuckerTVector{T, D} <: TVector

Tangent space to the Tucker manifold at $x = (U_1,\dots,U_D) â‹… \mathcal{C}$. This vector is
represented as
```math
(U_1,\dots,U_D) \cdot \dot{\mathcal{C}} +
\sum_{d=1}^D (U_1,\dots,U_{d-1},\dot{U}_d,U_{d+1},\dots,U_D) \cdot \mathcal{C}
```
where $\dot{U}_d^\mathrm{H} U_d = 0$. See also [`Tucker`](@ref)
"""
struct TuckerTVector{T,D} <: TVector
    CÌ‡::Array{T,D}
    UÌ‡::NTuple{D,Matrix{T}}
end

# An implicitly stored basis of the tangent space to the Tucker manifold. This is the basis
# from [Dewaele2021] and acts as the default orthonormal basis.
struct HOSVDBasis{T,D}
    point::TuckerPoint{T,D}
    UâŠ¥::NTuple{D,Matrix{T}}
end
CachedHOSVDBasis{ğ”½,T,D} =
    CachedBasis{ğ”½,DefaultOrthonormalBasis{ğ”½,TangentSpaceType},HOSVDBasis{T,D}}

âŠ—á´¿(a...) = Kronecker.:âŠ—(reverse(a)...)

Base.:*(s::Number, x::TuckerTVector) = TuckerTVector(s * x.CÌ‡, s .* x.UÌ‡)
Base.:*(x::TuckerTVector, s::Number) = TuckerTVector(x.CÌ‡ * s, x.UÌ‡ .* s)
Base.:/(x::TuckerTVector, s::Number) = TuckerTVector(x.CÌ‡ / s, x.UÌ‡ ./ s)
Base.:\(s::Number, x::TuckerTVector) = TuckerTVector(s \ x.CÌ‡, s .\ x.UÌ‡)
Base.:+(x::TuckerTVector, y::TuckerTVector) = TuckerTVector(x.CÌ‡ + y.CÌ‡, x.UÌ‡ .+ y.UÌ‡)
Base.:-(x::TuckerTVector, y::TuckerTVector) = TuckerTVector(x.CÌ‡ - y.CÌ‡, x.UÌ‡ .- y.UÌ‡)
Base.:-(x::TuckerTVector) = TuckerTVector(-x.CÌ‡, map(-, x.UÌ‡))
Base.:+(x::TuckerTVector) = TuckerTVector(x.CÌ‡, x.UÌ‡)
Base.:(==)(x::TuckerTVector, y::TuckerTVector) = (x.CÌ‡ == y.CÌ‡) && all(x.UÌ‡ .== y.UÌ‡)

allocate(p::TuckerPoint) = allocate(p, number_eltype(p))
function allocate(p::TuckerPoint{Tp,D}, ::Type{T}) where {T,Tp,D}
    @assert promote_type(Tp, T) == T
    return TuckerPoint(
        HOSVD(allocate(p.hosvd.U, T), allocate(p.hosvd.core, T), allocate(p.hosvd.Ïƒ, T)),
    )
end
allocate(x::TuckerTVector) = allocate(x, number_eltype(x))
function allocate(x::TuckerTVector, ::Type{T}) where {T}
    return TuckerTVector(allocate(x.CÌ‡, T), allocate(x.UÌ‡, T))
end

# Tuple-like broadcasting of TuckerTVector
Base.axes(::TuckerTVector) = ()

function Broadcast.BroadcastStyle(::Type{TuckerTVector{T,D}}) where {T,D}
    return Broadcast.Style{TuckerTVector{Any,D}}()
end
function Broadcast.BroadcastStyle(
    ::Broadcast.AbstractArrayStyle{0},
    b::Broadcast.Style{<:TuckerTVector},
)
    return b
end

function Broadcast.instantiate(
    bc::Broadcast.Broadcasted{Broadcast.Style{TuckerTVector{Any,D}},Nothing},
) where {D}
    return bc
end
function Broadcast.instantiate(
    bc::Broadcast.Broadcasted{Broadcast.Style{TuckerTVector{Any,D}}},
) where {D}
    Broadcast.check_broadcast_axes(bc.axes, bc.args...)
    return bc
end

Broadcast.broadcastable(v::TuckerTVector) = v

Base.@propagate_inbounds function Broadcast._broadcast_getindex(
    v::TuckerTVector,
    ::Val{I},
) where {I}
    if I isa Symbol
        return getfield(v, I)
    else
        return getfield(v, I[1])[I[2]]
    end
end

####

@doc raw"""
check_point(M::Tucker{N,R,D}, x; kwargs...) where {N,R,D}

Check whether the array or [`TuckerPoint`](@ref) x is a point on the [`Tucker`](@ref)
manifold, i.e. it is an $N_1 \times \dots \times N_D$ tensor of multilinear rank
$(R_1,\dots,R_D)$. The keyword arguments are passed to the matrix rank function applied to
the unfoldings.
For a [`TuckerPoint`](@ref) it is checked that the point is in correct HOSVD form.
"""
function check_point(M::Tucker{N,R,D}, x; kwargs...) where {N,R,D}
    s = "The point $(x) does not lie on $(M), "
    size(x) == N || return DomainError(size(x), s * "since its size is not $(N).")
    x_buffer = similar(x)
    for d in 1:ndims(x)
        r = rank(tensor_unfold!(x_buffer, x, d); kwargs...)
        r == R[d] || return DomainError(size(x), s * "since its rank is not $(R).")
    end
    return nothing
end
function check_point(M::Tucker{N,R,D}, x::TuckerPoint; kwargs...) where {N,R,D}
    s = "The point $(x) does not lie on $(M), "
    U = x.hosvd.U
    â„­ = x.hosvd.core
    if size(â„­) â‰  R
        return DomainError(
            size(x.hosvd.core),
            s * "since the size of the core is not $(R).",
        )
    end
    if size(x) â‰  N
        return DomainError(size(x), s * "since its dimensions are not $(N).")
    end
    for u in U
        if u' * u â‰‰ LinearAlgebra.I
            return DomainError(
                norm(u' * u - LinearAlgebra.I),
                s * "since its factor matrices are not unitary.",
            )
        end
    end
    â„­_buffer = similar(â„­)
    for d in 1:ndims(x.hosvd.core)
        â„­â½áµˆâ¾ = tensor_unfold!(â„­_buffer, â„­, d)
        gram = â„­â½áµˆâ¾ * â„­â½áµˆâ¾'
        if gram â‰‰ Diagonal(x.hosvd.Ïƒ[d])^2
            return DomainError(
                norm(gram - Diagonal(x.hosvd.Ïƒ[d])^2),
                s *
                "since the unfoldings of the core are not diagonalised by" *
                "the singular values.",
            )
        end
        if rank(Diagonal(x.hosvd.Ïƒ[d]); kwargs...) â‰  R[d]
            return DomainError(
                minimum(x.hosvd.Ïƒ[d]),
                s * "since the core does not have full multilinear rank.",
            )
        end
    end
    return nothing
end

@doc raw"""
check_vector(M::Tucker{N,R,D}, p::TuckerPoint{T,D}, v::TuckerTVector) where {N,R,T,D}

Check whether a [`TuckerTVector`](@ref) `v` is is in the tangent space to `M` at `p`. This
is the case when the dimensions of the factors in `v` agree with those of `p` and the factor
matrices of `v` are in the orthogonal complement of the HOSVD factors of `p`.
"""
function check_vector(
    M::Tucker{N,R,D},
    p::TuckerPoint{T,D},
    v::TuckerTVector,
) where {N,R,T,D}
    s = "The tangent vector $(v) is not a tangent vector to $(p) on $(M), "
    if size(p.hosvd.core) â‰  size(v.CÌ‡) || any(size.(v.UÌ‡) .â‰  size.(p.hosvd.U))
        return DomainError(
            size(v.CÌ‡),
            s * "since the array dimensons of $(p) and $(v)" * "do not agree.",
        )
    end
    for (U, UÌ‡) in zip(p.hosvd.U, v.UÌ‡)
        if norm(U' * UÌ‡) â‰¥ âˆšeps(eltype(U)) * âˆšlength(U)
            return DomainError(
                norm(U' * UÌ‡),
                s *
                "since the columns of x.hosvd.U are not" *
                "orthogonal to those of v.UÌ‡.",
            )
        end
    end
    return nothing
end

"""
Base.convert(::Type{Matrix{T}}, basis :: CachedBasis{ğ”½,DefaultOrthonormalBasis{ğ”½, TangentSpaceType},HOSVDBasis{T, D}}) where {ğ”½, T, D}
Base.convert(::Type{Matrix}, basis :: CachedBasis{ğ”½,DefaultOrthonormalBasis{ğ”½, TangentSpaceType},HOSVDBasis{T, D}}) where {ğ”½, T, D}

Convert a HOSVD basis to a matrix whose columns are the vectorisations of the basis vectors.
"""
function Base.convert(::Type{Matrix{T}}, â„¬::CachedHOSVDBasis{ğ”½,T,D}) where {ğ”½,T,D}
    ğ”„ = â„¬.data.point
    râƒ— = size(ğ”„.hosvd.core)
    nâƒ— = size(ğ”„)
    â„³ = Tucker(nâƒ—, râƒ—)

    J = Matrix{T}(undef, prod(nâƒ—), manifold_dimension(â„³))
    # compute all possible âˆ‚ğ”„â•±âˆ‚â„­ (in one go is quicker than one vector at a time)
    J[:, 1:prod(râƒ—)] = âŠ—á´¿(ğ”„.hosvd.U...)
    # compute all possible âˆ‚ğ”„â•±âˆ‚U[d] for d = 1,...,D
    function fill_column!(i, váµ¢)
        Jáµ¢_tensor = reshape(view(J, :, i), nâƒ—) # changes to this apply to J as well
        return embed!(â„³, Jáµ¢_tensor, ğ”„, váµ¢)
    end
    foreach(fill_column!, â„³, ğ”„, â„¬, (prod(râƒ—) + 1):manifold_dimension(â„³))
    return J
end
function Base.convert(::Type{Matrix}, basis::CachedHOSVDBasis{ğ”½,T,D}) where {ğ”½,T,D}
    return convert(Matrix{T}, basis)
end

@inline function Base.copy(
    bc::Broadcast.Broadcasted{Broadcast.Style{TuckerTVector{Any,D}}},
) where {D}
    return TuckerTVector(
        @inbounds(Broadcast._broadcast_getindex(bc, Val(:CÌ‡))),
        ntuple(i -> @inbounds(Broadcast._broadcast_getindex(bc, Val((:UÌ‡, i)))), Val(D)),
    )
end
Base.copy(x::TuckerTVector) = TuckerTVector(copy(x.CÌ‡), map(copy, x.UÌ‡))

function Base.copyto!(q::TuckerPoint, p::TuckerPoint)
    for d in 1:ndims(q)
        copyto!(q.hosvd.U[d], p.hosvd.U[d])
        copyto!(q.hosvd.Ïƒ[d], p.hosvd.Ïƒ[d])
    end
    copyto!(q.hosvd.core, p.hosvd.core)
    return q
end
function Base.copyto!(y::TuckerTVector, x::TuckerTVector)
    for d in 1:ndims(y.CÌ‡)
        copyto!(y.UÌ‡[d], x.UÌ‡[d])
    end
    copyto!(y.CÌ‡, x.CÌ‡)
    return y
end
@inline function Base.copyto!(
    dest::TuckerTVector,
    bc::Broadcast.Broadcasted{Broadcast.Style{TuckerTVector{Any,D}}},
) where {D}
    # Performance optimization: broadcast!(identity, dest, A) is equivalent to copyto!(dest, A) if indices match
    if bc.f === identity && bc.args isa Tuple{TuckerTVector} # only a single input argument to broadcast!
        A = bc.args[1]
        return copyto!(dest, A)
    end
    bcâ€² = Broadcast.preprocess(dest, bc)
    copyto!(dest.CÌ‡, Broadcast._broadcast_getindex(bcâ€², Val(:CÌ‡)))
    for i in 1:D
        copyto!(dest.UÌ‡[i], Broadcast._broadcast_getindex(bc, Val((:UÌ‡, i))))
    end
    return dest
end

@doc raw"""
embed(::Tucker, A :: TuckerPoint)

Convert a point `A` on the Tucker manifold to a full tensor, represented as an
$N_1 \times \dots \times N_D$-array.

embed(::Tucker, A::TuckerPoint, X::TuckerTVector)

Convert a tangent vector `X` with base point `A` on the Tucker manifold to a full tensor,
represented as an $N_1 \times \dots \times N_D$-array.
"""
embed(::Tucker, ::Any, ::TuckerPoint)

function embed!(::Tucker, q, p::TuckerPoint)
    return copyto!(q, reshape(âŠ—á´¿(p.hosvd.U...) * vec(p.hosvd.core), size(p)))
end
function embed!(â„³::Tucker, Y, ğ”„::TuckerPoint{T,D}, X::TuckerTVector) where {T,D}
    mul!(vec(Y), âŠ—á´¿(ğ”„.hosvd.U...), vec(X.CÌ‡))
    ğ”„_embedded = embed(â„³, ğ”„)
    buffer = similar(ğ”„_embedded)
    for k in 1:D
        UÌ‡â‚–Uâ‚–áµ€ğ”„â‚â‚–â‚ = X.UÌ‡[k] * (ğ”„.hosvd.U[k]' * tensor_unfold!(buffer, ğ”„_embedded, k))
        Y .= Y + tensor_fold!(buffer, UÌ‡â‚–Uâ‚–áµ€ğ”„â‚â‚–â‚, k)
    end
    return Y
end

@doc raw"""
Base.foreach(f, M::Tucker, p::TuckerPoint, basis::AbstractBasis, indices=1:manifold_dimension(M))

Let `basis` be and [`AbstractBasis`](@ref) at a point `p` on `M`. Suppose `f` is a function
that takes an index and a vector as an argument.
This function applies `f` to `i` and the `i`th basis vector sequentially for each `i` in
`indices`.
Using a [`CachedBasis`](@ref) may speed up the computation.

**NOTE**: The i'th basis vector is overwritten in each iteration. If any information about
the vector is to be stored, `f` must make a copy.
"""
function Base.foreach(
    f,
    M::Tucker,
    p::TuckerPoint,
    basis::AbstractBasis,
    indices=1:manifold_dimension(M),
)
    # Use mutating variants to avoid superfluous allocation
    báµ¢ = zero_vector(M, p)
    eáµ¢ = zeros(number_eltype(p), manifold_dimension(M))
    for i in indices
        eáµ¢[i] = one(eltype(eáµ¢))
        get_vector!(M, báµ¢, p, eáµ¢, basis)
        eáµ¢[i] = zero(eltype(eáµ¢))
        f(i, báµ¢)
    end
end

@doc raw"""
get_basis(:: Tucker, A :: TuckerPoint, basisType::DefaultOrthonormalBasis{ğ”½, TangentSpaceType}) where ğ”½

A implicitly stored basis of the tangent space to the Tucker manifold.
Assume $\mathcal{A} = (U_1,\dots,U_D) \cdot \mathcal{C}$ is in HOSVD format and that, for
$d=1,\dots,D$, the singular values of the
$d$'th unfolding are $\sigma_{dj}$, with $j = 1,\dots,R_d$.
The basis of the tangent space is as follows: [^Dewaele2021]

```math
\bigl\{
(U_1,\dots,U_D) e_i
\bigr\} \cup \bigl\{
(U_1,\dots, \sigma_{dj}^{-1} U_d^{\perp} e_i e_j^T,\dots,U_D) \cdot \mathcal{C}
\bigr\}
```

in which $U_d^\perp$ is such that $[U_d \quad U_d^{\perp}]$ forms an orthonormal basis
of $\mathbb{R}^{N_d}$, for each $d = 1,\dots,D$.

[^Dewaele2021]:
> Nick Dewaele, Paul Breiding, Nick Vannieuwenhoven, "The condition number of many tensor decompositions is invariant under Tucker compression"
> arxiv: [2106.13034](https://arxiv.org/abs/2106.13034)
"""
function get_basis(
    ::Tucker,
    ğ”„::TuckerPoint,
    basisType::DefaultOrthonormalBasis{ğ”½,TangentSpaceType}=DefaultOrthonormalBasis(),
) where {ğ”½}
    D = ndims(ğ”„)
    nâƒ— = size(ğ”„)
    râƒ— = size(ğ”„.hosvd.core)

    U = ğ”„.hosvd.U
    UâŠ¥ = ntuple(d -> Matrix(qr(I - U[d] * U[d]', Val(true)).Q)[:, 1:(nâƒ—[d] - râƒ—[d])], D)

    basis = HOSVDBasis(ğ”„, UâŠ¥)
    return CachedBasis(basisType, basis)
end

#=
get_coordinates(::Tucker, A, X :: TuckerTVector, b)

The coordinates of a tangent vector X at point A on the Tucker manifold with respect to the
basis b.
=#
function get_coordinates(::Tucker, ğ”„, X::TuckerTVector, â„¬::CachedHOSVDBasis)
    coords = vec(X.CÌ‡)
    for d in 1:length(X.UÌ‡)
        coord_mtx = (â„¬.data.UâŠ¥[d] \ X.UÌ‡[d]) * Diagonal(ğ”„.hosvd.Ïƒ[d])
        coords = vcat(coords, vec(coord_mtx'))
    end
    return coords
end
function get_coordinates(
    M::Tucker,
    ğ”„,
    X,
    â„¬::DefaultOrthonormalBasis{ğ”½,TangentSpaceType},
) where {ğ”½}
    return get_coordinates(M, ğ”„, X, get_basis(M, ğ”„, â„¬))
end

#=
get_vector(::Tucker, A, x, b)

The tangent vector at a point A whose coordinates with respect to the basis b are x.
=#
function get_vector!(
    ::Tucker,
    y,
    ğ”„::TuckerPoint,
    x::AbstractVector{T},
    â„¬::CachedHOSVDBasis,
) where {T}
    Î¾ = convert(Vector{promote_type(number_eltype(ğ”„), eltype(x))}, x)
    â„­ = ğ”„.hosvd.core
    Ïƒ = ğ”„.hosvd.Ïƒ
    UâŠ¥ = â„¬.data.UâŠ¥
    D = ndims(â„­)
    râƒ— = size(â„­)
    nâƒ— = size(ğ”„)

    # split Î¾ into Î¾_core and Î¾U so that vcat(Î¾_core, Î¾U...) == Î¾, but avoid copying
    Î¾_core = view(Î¾, 1:length(â„­))
    Î¾U = Vector{typeof(Î¾_core)}(undef, D)
    nextcolumn = length(â„­) + 1
    for d in 1:D
        numcols = râƒ—[d] * (nâƒ—[d] - râƒ—[d])
        Î¾U[d] = view(Î¾, nextcolumn:(nextcolumn + numcols - 1))
        nextcolumn += numcols
    end

    # Construct âˆ‚U[d] by plugging in the definition of the orthonormal basis [Dewaele2021]
    # âˆ‚U[d] = âˆ‘áµ¢â±¼ { Î¾U[d]áµ¢â±¼ (Ïƒ[d]â±¼)â»Â¹ UâŠ¥[d] ğáµ¢ ğâ±¼áµ€ }
    #       = UâŠ¥[d] * âˆ‘â±¼ (Ïƒ[d]â±¼)â»Â¹ (âˆ‘áµ¢ Î¾U[d]áµ¢â±¼  ğáµ¢) ğâ±¼áµ€
    # Î¾U[d] = [Î¾â‚â‚, ..., Î¾â‚â±¼, ..., Î¾áµ¢â‚, ..., Î¾áµ¢â±¼, ..., ]
    # => turn these i and j into matrix indices and do matrix operations
    for d in 1:D
        grid = transpose(reshape(Î¾U[d], râƒ—[d], nâƒ—[d] - râƒ—[d]))
        mul!(y.UÌ‡[d], UâŠ¥[d], grid * Diagonal(1 ./ Ïƒ[d]))
    end

    y.CÌ‡ .= reshape(Î¾_core, size(y.CÌ‡))
    return y
end
function get_vector!(
    â„³::Tucker,
    y,
    ğ”„::TuckerPoint,
    x,
    â„¬::DefaultOrthonormalBasis{ğ”½,TangentSpaceType},
) where {ğ”½}
    return get_vector!(â„³, y, ğ”„, x, get_basis(â„³, ğ”„, â„¬))
end

function get_vectors(â„³::Tucker, ğ”„::TuckerPoint{T,D}, â„¬::CachedHOSVDBasis) where {T,D}
    vectors = Vector{TuckerTVector{T,D}}(undef, manifold_dimension(â„³))
    foreach((i, váµ¢) -> setindex!(vectors, copy(váµ¢), i), â„³, ğ”„, â„¬)
    return vectors
end
function get_vectors(â„³::Tucker, ğ”„::TuckerPoint, â„¬::DefaultOrthonormalBasis)
    return get_vectors(â„³, ğ”„, get_basis(â„³, ğ”„, â„¬))
end

"""
inner(::Tucker, A::TuckerPoint, x::TuckerTVector, y::TuckerTVector)

The Euclidean inner product between tangent vectors `x` and `y` at the point `A` on
the Tucker manifold.function

inner(::Tucker, A::TuckerPoint, x::TuckerTVector, y)
inner(::Tucker, A::TuckerPoint, x, y::TuckerTVector)

The Euclidean inner product between `x` and `y` where `x` is a vector tangent to the Tucker
manifold at `A` and `y` is a vector or the ambient space or vice versa. The vector in the
ambient space is represented as a full tensor.
"""
function inner(::Tucker, ğ”„::TuckerPoint, x::TuckerTVector, y::TuckerTVector)
    â„­ = ğ”„.hosvd.core
    dotprod = dot(x.CÌ‡, y.CÌ‡)
    â„­_buffer = similar(â„­)
    for k in 1:ndims(ğ”„)
        â„­â‚â‚–â‚ = tensor_unfold!(â„­_buffer, â„­, k)
        dotprod += dot(x.UÌ‡[k] * â„­â‚â‚–â‚, y.UÌ‡[k] * â„­â‚â‚–â‚)
    end
    return dotprod
end
inner(M::Tucker, ğ”„::TuckerPoint, x::TuckerTVector, y) = dot(embed(M, ğ”„, x), y)
inner(M::Tucker, ğ”„::TuckerPoint, x, y::TuckerTVector) = dot(x, embed(M, ğ”„, y))

"""
inverse_retract(â„³::Tucker, A::TuckerPoint, B::TuckerPoint, r::ProjectionInverseRetraction)

The projection inverse retraction on the Tucker manifold interprets `B` as a point in the
ambient Euclidean space and projects it onto the tangent space at to `â„³` at `A`.
"""
inverse_retract(
    ::Tucker,
    ::Any,
    ::TuckerPoint,
    ::TuckerPoint,
    ::ProjectionInverseRetraction,
)

function inverse_retract!(
    â„³::Tucker,
    X,
    ğ”„::TuckerPoint,
    ğ”…::TuckerPoint,
    ::ProjectionInverseRetraction,
)
    diffVector = embed(â„³, ğ”…) - embed(â„³, ğ”„)
    return project!(â„³, X, ğ”„, diffVector)
end

function isapprox(p::TuckerPoint, q::TuckerPoint; kwargs...)
    â„³ = Tucker(size(p), size(p.hosvd.core))
    return isapprox(embed(â„³, p), embed(â„³, q); kwargs...)
end
isapprox(::Tucker, p::TuckerPoint, q::TuckerPoint; kwargs...) = isapprox(p, q; kwargs...)
function isapprox(M::Tucker, p::TuckerPoint, x::TuckerTVector, y::TuckerTVector; kwargs...)
    return isapprox(embed(M, p, x), embed(M, p, y); kwargs...)
end

#=
Determines whether there are tensors of dimensions nâƒ— with multilinear rank râƒ—
=#
function is_valid_mlrank(nâƒ—, râƒ—)
    return all(râƒ— .â‰¥ 1) &&
           all(râƒ— .â‰¤ nâƒ—) &&
           all(ntuple(i -> râƒ—[i] â‰¤ prod(râƒ—) Ã· râƒ—[i], length(râƒ—)))
end

@doc raw"""
manifold_dimension(::Tucker)

The dimension of the manifold of $N_1 \times \dots \times N_D$ tensors of multilinear
rank $R_1 \times \dots \times R_D$, i.e.
```math
\mathrm{dim}(\mathcal{M}) = \prod_{d=1}^D R_d + \sum_{d=1}^D R_d (N_d - R_d).
```
"""
manifold_dimension(::Tucker{nâƒ—,râƒ—}) where {nâƒ—,râƒ—} = prod(râƒ—) + sum(râƒ— .* (nâƒ— .- râƒ—))

@doc raw"""
Base.ndims(A :: TuckerPoint{T, D})

The order of the tensor corresponding to the [`TuckerPoint`](@ref) `A`
"""
Base.ndims(::TuckerPoint{T,D}) where {T,D} = D

number_eltype(::TuckerPoint{T,D}) where {T,D} = T
number_eltype(::TuckerTVector{T,D}) where {T,D} = T

"""
project(â„³::Tucker, ğ”„::TuckerPoint, X)

The least-squares projection of a tensor `X` to the tangent space to `â„³` at `A`.
"""
project(::Tucker, ::Any, ::TuckerPoint, ::Any)

function project!(â„³::Tucker, Y, ğ”„::TuckerPoint, X)
    â„¬ = get_basis(â„³, ğ”„, DefaultOrthonormalBasis())
    coords = Vector{number_eltype(ğ”„)}(undef, manifold_dimension(â„³))
    f!(i, â„¬áµ¢) = setindex!(coords, inner(â„³, ğ”„, â„¬áµ¢, X), i)
    foreach(f!, â„³, ğ”„, â„¬)
    return get_vector!(â„³, Y, ğ”„, coords, â„¬)
end

@doc raw"""
retract(::Tucker, A, x, ::PolarRetraction)

The truncated HOSVD-based retraction [^Kressner2014] to the Tucker manifold, i.e.
$R_{\mathcal{A}}(x)$ is the sequentially tuncated HOSVD of $\mathcal{A} + x$

In the exceptional case that the multilinear rank of $\mathcal{A} + x$ is lower than A, this
retraction produces a boundary point.

[^Kressner2014]:
> Daniel Kressner, Michael Steinlechner, Bart Vandereycken: "Low-rank tensor completion by Riemannian optimization"
> BIT Numerical Mathematics, 54(2), pp. 447-468, 2014
> doi: [10.1007/s10543-013-0455-z](https://doi.org/10.1007/s10543-013-0455-z)

"""
retract(::Tucker, ::Any, ::Any, ::PolarRetraction)

function retract!(
    ::Tucker,
    q::TuckerPoint,
    p::TuckerPoint{T,D},
    x::TuckerTVector,
    ::PolarRetraction,
) where {T,D}
    U = p.hosvd.U
    V = x.UÌ‡
    â„­ = p.hosvd.core
    ğ”Š = x.CÌ‡
    râƒ— = size(â„­)

    # Build the core tensor S and the factors [Uáµˆ  Váµˆ]
    S = zeros(T, 2 .* size(â„­))
    S[CartesianIndices(â„­)] = â„­ + ğ”Š
    UQ = Matrix{T}[]
    buffer = similar(â„­)
    for k in 1:D
        # We make the following adaptation to Kressner2014:
        # Fix the i'th term of the sum and replace Váµ¢ by Qáµ¢ Ráµ¢.
        # We can absorb the R factor into the core by replacing Váµ¢ by Qáµ¢
        # and C (in the i'th term of the sum) by C Ã—áµ¢ Ráµ¢
        Q, R = qr(V[k])
        idxOffset = CartesianIndex(ntuple(i -> i == k ? râƒ—[k] : 0, D))
        â„­â¨‰â‚–R = tensor_fold!(buffer, R * tensor_unfold!(buffer, â„­, k), k)
        S[CartesianIndices(â„­) .+ idxOffset] = â„­â¨‰â‚–R
        push!(UQ, hcat(U[k], Matrix(Q)))
    end

    #Convert to truncated HOSVD of p + x
    hosvd_S = st_hosvd(S, râƒ—)
    factors = UQ .* hosvd_S.U
    for i in 1:D
        q.hosvd.U[i] .= factors[i]
        q.hosvd.Ïƒ[i] .= hosvd_S.Ïƒ[i]
    end
    q.hosvd.core .= hosvd_S.core
    return q
end

function Base.show(io::IO, ::MIME"text/plain", ğ’¯::Tucker{N,R,D,ğ”½}) where {N,R,D,ğ”½}
    return print(io, "Tucker(", N, ", ", R, ", ", ğ”½, ")")
end
function Base.show(io::IO, ::MIME"text/plain", ğ”„::TuckerPoint)
    pre = " "
    summary(io, ğ”„)
    for d in eachindex(ğ”„.hosvd.U)
        println(io, string("\nU factor ", d, ":"))
        su = sprint(show, "text/plain", ğ”„.hosvd.U[d]; context=io, sizehint=0)
        su = replace(su, '\n' => "\n$(pre)")
        println(io, pre, su)
    end
    println(io, "\nCore:")
    su = sprint(show, "text/plain", ğ”„.hosvd.core; context=io, sizehint=0)
    su = replace(su, '\n' => "\n$(pre)")
    return print(io, pre, su)
end
function Base.show(io::IO, ::MIME"text/plain", x::TuckerTVector)
    pre = " "
    summary(io, x)
    for d in eachindex(x.UÌ‡)
        println(io, string("\nUÌ‡ factor ", d, ":"))
        su = sprint(show, "text/plain", x.UÌ‡[d]; context=io, sizehint=0)
        su = replace(su, '\n' => "\n$(pre)")
        println(io, pre, su)
    end
    println(io, "\nCÌ‡ factor:")
    su = sprint(show, "text/plain", x.CÌ‡; context=io, sizehint=0)
    su = replace(su, '\n' => "\n$(pre)")
    return print(io, pre, su)
end
function Base.show(io::IO, ::MIME"text/plain", â„¬::CachedHOSVDBasis{ğ”½,T,D}) where {ğ”½,T,D}
    summary(io, â„¬)
    print(io, " â‰…")
    su = sprint(show, "text/plain", convert(Matrix{T}, â„¬); context=io, sizehint=0)
    su = replace(su, '\n' => "\n ")
    return println(io, " ", su)
end

"""
Base.size(A::TuckerPoint)

The dimensions of a [`TuckerPoint`](@ref) `A`, when regarded as a full tensor
"""
Base.size(ğ”„::TuckerPoint) = map(u -> size(u, 1), ğ”„.hosvd.U)

#=
Modification of the ST-HOSVD from [Vannieuwenhoven2012]
This is the HOSVD of an approximation of ğ”„, i.e. the core of this decomposition
is also in HOSVD format.
=#
function st_hosvd(ğ”„, mlrank=size(ğ”„))
    T = eltype(ğ”„)
    D = ndims(ğ”„)
    nâƒ— = size(ğ”„)
    # Add type assertions to U and Ïƒ for type stability
    U::NTuple{D,Matrix{T}} = ntuple(d -> Matrix{T}(undef, nâƒ—[d], mlrank[d]), D)
    Ïƒ::NTuple{D,Vector{T}} = ntuple(d -> Vector{T}(undef, mlrank[d]), D)
    # Initialise arrays to store successive truncations (ğ”„â€²) and unfoldings (buffer)
    # so that the type remains constant at every truncation
    ğ”„â€² = reshape(view(ğ”„, 1:length(ğ”„)), nâƒ—)
    fold_buffer = reshape(view(similar(ğ”„), 1:length(ğ”„)), nâƒ—)
    unfold_buffer = view(similar(ğ”„), 1:length(ğ”„))

    for k in 1:D
        râ‚– = mlrank[k]
        ğ”„â€²â‚â‚–â‚ = tensor_unfold!(unfold_buffer, ğ”„â€², k)
        # truncated SVD + incremental construction of the core
        UÎ£Váµ€ = svd(ğ”„â€²â‚â‚–â‚)
        U[k] .= UÎ£Váµ€.U[:, 1:râ‚–]
        Ïƒ[k] .= UÎ£Váµ€.S[1:râ‚–]
        ğ”„â€²â‚â‚–â‚_trunc = Diagonal(Ïƒ[k]) * UÎ£Váµ€.Vt[1:râ‚–, :]
        sizeğ”„â€² = ntuple(i -> i â‰¤ k ? mlrank[i] : nâƒ—[i], D)
        fold_buffer = reshape(view(fold_buffer, 1:prod(sizeğ”„â€²)), sizeğ”„â€²)
        unfold_buffer = view(unfold_buffer, 1:prod(sizeğ”„â€²))
        ğ”„â€² = tensor_fold!(fold_buffer, ğ”„â€²â‚â‚–â‚_trunc, k)
    end
    core = Array(ğ”„â€²)

    # Make sure the truncated core is in "all-orthogonal" HOSVD format
    if mlrank â‰  nâƒ—
        hosvd_core = st_hosvd(core, mlrank)
        U = U .* hosvd_core.U
        core = hosvd_core.core
        Ïƒ = hosvd_core.Ïƒ
    end

    return HOSVD{T,D}(U, core, Ïƒ)
end

# In-place inverse of the k'th unfolding of a size nâ‚ Ã— ... Ã— n_D tensor.
# The size of the reshaped tensor is determined by the size of ğ”„.
# The result is stored in ğ”„. The returned value uses the same address space as ğ”„.
function tensor_fold!(ğ”„::AbstractArray{T,D}, ğ”„â‚â‚–â‚::AbstractMatrix{T}, k) where {T,D}
    @assert length(ğ”„â‚â‚–â‚) == length(ğ”„) && size(ğ”„â‚â‚–â‚, 1) == size(ğ”„, k)
    @assert pointer(ğ”„) !== pointer(ğ”„â‚â‚–â‚)
    # Caution: tuple operations can be type unstable if used incorrectly
    Ïƒ(i) = i == 1 ? k : i â‰¤ k ? i - 1 : i
    Ïƒâ»Â¹(i) = i < k ? i + 1 : i == k ? 1 : i
    permuted_size = ntuple(i -> size(ğ”„, Ïƒ(i)), D)
    return permutedims!(ğ”„, reshape(ğ”„â‚â‚–â‚, permuted_size), ntuple(Ïƒâ»Â¹, D))
end

# In-place mode-k unfolding of the array ğ”„ of order D â‰¥ k.
# The argument buffer is an array of arbitrary dimensions of the same length as ğ”„.
# The returned value uses the same address space as the buffer.
function tensor_unfold!(buffer, ğ”„::AbstractArray{T,D}, k) where {T,D}
    @assert length(buffer) == length(ğ”„)
    @assert pointer(ğ”„) !== pointer(buffer)
    ğ”„â‚â‚–â‚ = reshape(buffer, size(ğ”„, k), :)
    # Caution: tuple operations can be type unstable if used incorrectly
    Ïƒ(i) = i == 1 ? k : i â‰¤ k ? i - 1 : i
    permuted_size = ntuple(i -> size(ğ”„, Ïƒ(i)), D)
    permutedims!(reshape(ğ”„â‚â‚–â‚, permuted_size), ğ”„, ntuple(Ïƒ, D))
    return ğ”„â‚â‚–â‚
end

@doc raw"""
zero_vector(::Tucker, A::TuckerPoint)

The zero element in the tangent space to A on the Tucker manifold, represented as a
[`TuckerTVector`](@ref)
"""
function zero_vector!(::Tucker, X::TuckerTVector, ::TuckerPoint)
    for UÌ‡ in X.UÌ‡
        fill!(UÌ‡, zero(eltype(UÌ‡)))
    end
    fill!(X.CÌ‡, zero(eltype(X.CÌ‡)))
    return X
end

# The standard implementation of allocate_result on vector-valued functions gives an element
# of the same type as the manifold point. We want a vector instead.
for fun in [:get_vector, :inverse_retract, :project, :zero_vector]
    @eval function ManifoldsBase.allocate_result(
        ::Tucker,
        ::typeof($(fun)),
        p::TuckerPoint,
        args...,
    )
        return TuckerTVector(allocate(p.hosvd.core), allocate(p.hosvd.U))
    end
end

function ManifoldsBase.allocate_result(::Tucker{N}, f::typeof(embed), p, args...) where {N}
    dims = N
    return Array{number_eltype(p),length(dims)}(undef, dims)
end
